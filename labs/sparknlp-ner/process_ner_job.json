{
  "type" : "parallel-bulk-loader",
  "id" : "sparknlp_ner_extraction_job_config",
  "format" : "parquet",
  "path" : "s3a://sstk-dev-1/data/sparknlp_ner_extraction_data.parquet",
  "outputCollection" : "sparknlp_ner_extraction",
  "clearDatasource" : false,
  "defineFieldsUsingInputSchema" : true,
  "atomicUpdates" : false,
  "transformScala" : "\nimport com.johnsnowlabs.nlp.annotator._\nimport com.johnsnowlabs.nlp.annotators.ner.NerConverter\nimport com.johnsnowlabs.nlp.base._\nimport com.johnsnowlabs.util.Benchmark\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.sql.SparkSession\nimport spark.implicits._\n\ndef transform(inputDF: Dataset[Row]): Dataset[Row] = {\n\n  val document = new DocumentAssembler()\n    .setInputCol(\"text\")\n    .setOutputCol(\"document\")\n  val token = new Tokenizer()\n    .setInputCols(\"document\")\n    .setOutputCol(\"token\")\n  val normalizer = new Normalizer()\n    .setInputCols(\"token\")\n    .setOutputCol(\"normal\")\n  val ner = NerDLModel.pretrained()\n    .setInputCols(\"normal\", \"document\")\n    .setOutputCol(\"ner\")\n  val nerConverter = new NerConverter()\n    .setInputCols(\"document\", \"normal\", \"ner\")\n    .setOutputCol(\"ner_converter\")\n  val finisher = new Finisher()\n    .setInputCols(\"ner\", \"ner_converter\")\n    .setIncludeMetadata(true)\n    .setOutputAsArray(false)\n    .setCleanAnnotations(false)\n    .setAnnotationSplitSymbol(\"@\")\n    .setValueSplitSymbol(\"#\")\n\n  val pipeline = new Pipeline().\n    setStages(Array(document, token, normalizer, ner, nerConverter, finisher))\n  val intext = inputDF.toDF\n  val result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(intext)\n  val only_ner = result.select(\"text\",\"finished_ner\")\n\n  /* --- some regex/pattern magic to extract the entities and tags from each sentence ---\n   sent is of form: word->But#result->O@word->China#result->I-LOC@word->Asian#result->I-MISC\n   extracted entities: But -> 0, China -> I-LOC, Asian -> I-MISC  */\n  val tagRegex = raw\"word->(.*)#result->(.*)\".r\n  val entityRegex = raw\"([^@]*)\".r\n\n  val entitySplitter =  (sent: String) => entityRegex.findAllIn(sent).toArray\n  val tagYielder = (sent: String) => for(tagRegex(a,b) <- entitySplitter(sent)) yield(a,b)\n  /* -- end regex/pattern matching  -- */\n\n  val ner_by_tokens = only_ner.map(s =>  (s.getAs[String](0), tagYielder(s.getAs[String](1))))\n\n  val ner_words_rdd = ner_by_tokens.rdd\n  val mapped = ner_words_rdd.map{s => (s._1, s._2.groupBy{k => k._2})}\n  val mapped_pruned = mapped.map(s => (s._1, s._2.mapValues(_.map(_._1)).map(identity)))\n  var backToDf = spark.createDataFrame(mapped_pruned)\n  val oUDF = udf((pair: (Map[String, Seq[String]])) => pair.getOrElse(\"O\", Seq.empty[String]))\n  val lUDF = udf((pair: (Map[String, Seq[String]])) => pair.getOrElse(\"I-LOC\", Seq.empty[String]))\n  val pUDF = udf((pair: (Map[String, Seq[String]])) => pair.getOrElse(\"I-PER\", Seq.empty[String]))\n  val mUDF = udf((pair: (Map[String, Seq[String]])) => pair.getOrElse(\"I-MISC\", Seq.empty[String]))\n  val gUDF = udf((pair: (Map[String, Seq[String]])) => pair.getOrElse(\"I-ORG\", Seq.empty[String]))\n  val finalDF = backToDf.withColumn(\"O\",oUDF(col(\"_2\")))\n    .withColumn(\"LOC\", lUDF(col(\"_2\")))\n    .withColumn(\"PER\", pUDF(col(\"_2\")))\n    .withColumn(\"MISC\", mUDF(col(\"_2\")))\n    .withColumn(\"ORG\", gUDF(col(\"_2\")))\n    .drop(col(\"_2\"))\n    .withColumnRenamed(\"_1\",\"text\")\n  finalDF\n}",
  "shellOptions" : [ {
    "key" : "--packages",
    "value" : "JohnSnowLabs:spark-nlp:1.6.0"
  } ]
}
