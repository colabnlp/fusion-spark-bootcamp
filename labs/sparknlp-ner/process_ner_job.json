{
  "type" : "parallel-bulk-loader",
  "id" : "sparknlp_ner_extraction",
  "format" : "parquet",
  "path" : "s3a://fusion-spark-bootcamp/sparknlp_ner_extraction_data.parquet",
  "outputCollection" : "sparknlp_ner_extraction",
  "outputIndexPipeline" : "sparknlp_ner_extraction",
  "clearDatasource" : true,
  "defineFieldsUsingInputSchema" : true,
  "atomicUpdates" : false,
  "transformSql" : "select monotonically_increasing_id() as id, text, ner_json from _input",
  "transformScala" : "import com.johnsnowlabs.nlp.annotator._\nimport com.johnsnowlabs.nlp.annotators.ner.NerConverter\nimport com.johnsnowlabs.nlp.base._\nimport com.johnsnowlabs.util.Benchmark\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions.udf\nimport spark.implicits._\nimport scala.collection.mutable.HashMap\nimport scala.collection.mutable.HashSet\nimport scala.collection.mutable.WrappedArray\n\nimport com.fasterxml.jackson.databind.{DeserializationFeature, ObjectMapper}\nimport com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper\nimport com.fasterxml.jackson.module.scala.DefaultScalaModule\n\nobject JsonUtil {\n  val mapper = new ObjectMapper() with ScalaObjectMapper\n  mapper.registerModule(DefaultScalaModule)\n  mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false)\n  def toJson(value: Any): String = {\n    mapper.writeValueAsString(value)\n  }\n}\n\ndef extract_ner(a:WrappedArray[Row]) : String = {\n  val map = new HashMap[String,HashSet[String]]()\n  a.foreach(next => {\n    val et = next.getMap(next.fieldIndex(\"metadata\")).getOrElse(\"entity\",\"\")\n    if (et.nonEmpty) {\n      var list = map.get(et)\n      if (list.isEmpty) {\n        list = Some(new HashSet[String]())\n        map += (et -> list.get) \n      }\n      list.get += next.getString(next.fieldIndex(\"result\"))\n    } \n  })\n  JsonUtil.toJson(map)\n}\n\nval nerUDF = udf(extract_ner _)\n\ndef transform(inputDF: Dataset[Row]): Dataset[Row] = {\n  val document = new DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n  val sentenceDetector = new SentenceDetector().setInputCols(\"document\").setOutputCol(\"sentence\")\n  val token = new Tokenizer().setInputCols(\"sentence\").setOutputCol(\"token\")\n  val normalizer = new Normalizer().setInputCols(\"token\").setOutputCol(\"normal\")\n  val ner = NerDLModel.pretrained().setInputCols(\"normal\", \"sentence\").setOutputCol(\"ner\")\n  val nerConverter = new NerConverter().setInputCols(\"document\", \"normal\", \"ner\").setOutputCol(\"ner_converter\")\n  val finisher = new Finisher().setInputCols(\"ner\", \"ner_converter\").setIncludeMetadata(true)\n     .setOutputAsArray(false).setCleanAnnotations(false).setAnnotationSplitSymbol(\"@\").setValueSplitSymbol(\"#\")\n  val pipeline = new Pipeline().setStages(Array(document, sentenceDetector, token, normalizer, ner, nerConverter, finisher))\n  val result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(inputDF.filter(inputDF(\"text\").isNotNull))\n  result.select(\"text\",\"ner_converter\").withColumn(\"ner_json\",nerUDF($\"ner_converter\")).drop(\"ner_converter\")\n}",
  "shellOptions" : [ {
    "key" : "--packages",
    "value" : "JohnSnowLabs:spark-nlp:1.6.0"
  } ],
  "cacheAfterRead" : false
}
