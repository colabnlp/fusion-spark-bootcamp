{
  "type" : "parallel-bulk-loader",
  "id" : "sparknlp_ner_extraction_job_config",
  "format" : "parquet",
  "path" : "s3a://sstk-dev/data/sparknlp_ner_extraction_data.parquet",
  "outputCollection" : "sparknlp_ner_extraction",
  "clearDatasource" : false,
  "defineFieldsUsingInputSchema" : true,
  "atomicUpdates" : false,
  "transformScala" : "import org.apache.spark.sql.{DataFrame, Dataset, Row}\nimport com.johnsnowlabs.nlp.annotator._\nimport com.johnsnowlabs.nlp.annotators.ner.NerConverter\nimport com.johnsnowlabs.nlp.base._\nimport com.johnsnowlabs.util.Benchmark\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions.udf\nimport spark.implicits._\n\ndef splitTokens(str: String): List[String] = {\n  val foo = str.split(\"@\")\n  return foo.toList\n}\n\ndef splitWords(str: String): (String, String) = {\n  // str is of format: word->London#result->LOC\n  val xs = str.split(\"#\")\n  var first = \"\"\n  var secnd = \"\"\n  try {\n    first = xs(0).substring(6)\n    secnd = xs(1).substring(8)\n  } catch {\n    case e: Exception => {\n      first = \"ERROR: \" + str\n      secnd = \"ERROR: \" + str\n    }\n  }\n  (first, secnd)\n}\n\ndef massageList(vals: List[String]): List[(String, String)] = {\n  for (e <- vals)\n      yield splitWords(e)\n}\n\ndef transform(inputDF: Dataset[Row]): Dataset[Row] = {\n\n  val document = new DocumentAssembler()\n    .setInputCol(\"text\")\n    .setOutputCol(\"document\")\n\n  val token = new Tokenizer()\n    .setInputCols(\"document\")\n    .setOutputCol(\"token\")\n\n  val normalizer = new Normalizer()\n    .setInputCols(\"token\")\n    .setOutputCol(\"normal\")\n\n  val ner = NerDLModel.pretrained()\n    .setInputCols(\"normal\", \"document\")\n    .setOutputCol(\"ner\")\n\n  val nerConverter = new NerConverter()\n    .setInputCols(\"document\", \"normal\", \"ner\")\n    .setOutputCol(\"ner_converter\")\n\n  val finisher = new Finisher()\n    .setInputCols(\"ner\", \"ner_converter\")\n    .setIncludeMetadata(true)\n    .setOutputAsArray(false)\n    .setCleanAnnotations(false)\n\n  val pipeline = new Pipeline().\n    setStages(Array(document, token, normalizer, ner, nerConverter, finisher))\n\n\n\n  val intext = inputDF.toDF\n  val result = pipeline.fit(Seq.empty[String].toDS.toDF(\"text\")).transform(intext)\n  val only_ner = result.select(\"text\",\"finished_ner\")\n  val ner_by_tokens = only_ner.map(s => (s.getAs[String](0), splitTokens(s.getAs[String](1))))\n  val ner_by_words = ner_by_tokens.map(s => (s._1, massageList(s._2)))\n  val ner_words_rdd = ner_by_words.rdd\n  val mapped = ner_words_rdd.map{s => (s._1, s._2.groupBy{k => k._2})}\n\n  val mapped_pruned = mapped.map(s => (s._1, s._2.mapValues(_.map(_._1)).map(identity)))\n  var backToDf = spark.createDataFrame(mapped_pruned)\n\n  val oUDF = udf((pair: (Map[String, Seq[String]])) => pair.getOrElse(\"O\", Seq.empty[String]))\n  val lUDF = udf((pair: (Map[String, Seq[String]])) => pair.getOrElse(\"I-LOC\", Seq.empty[String]))\n  val pUDF = udf((pair: (Map[String, Seq[String]])) => pair.getOrElse(\"I-PER\", Seq.empty[String]))\n  val mUDF = udf((pair: (Map[String, Seq[String]])) => pair.getOrElse(\"I-MISC\", Seq.empty[String]))\n  val gUDF = udf((pair: (Map[String, Seq[String]])) => pair.getOrElse(\"I-ORG\", Seq.empty[String]))\n\n  val finalDF = backToDf.withColumn(\"O\",oUDF(col(\"_2\")))\n                        .withColumn(\"LOC\", lUDF(col(\"_2\")))\n                        .withColumn(\"PER\", pUDF(col(\"_2\")))\n                        .withColumn(\"MISC\", mUDF(col(\"_2\")))\n                        .withColumn(\"ORG\", gUDF(col(\"_2\")))\n                        .drop(col(\"_2\")).withColumnRenamed(\"_1\",\"text\")\n  finalDF\n}",
  "shellOptions" : [ {
    "key" : "--packages",
    "value" : "JohnSnowLabs:spark-nlp:1.5.5"
  } ]
}
